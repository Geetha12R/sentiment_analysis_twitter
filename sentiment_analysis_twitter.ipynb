{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Sentiment Classification\n",
    "\n",
    "For this exercise we will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following [link](https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2). We will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import necessary packages\n",
    "import nltk\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset) #{tweet id : sentiment}\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_process(text):\n",
    "    processed = re.sub(r\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \"\", text)\n",
    "    # processed = re.sub(r\"((https?|ftp)://)?[a-z0-9\\-._~:/?#\\[\\]@!$&'()*+,;=%]+\\.[a-z]{2,}[a-z0-9\\-._~:/?#\\[\\]@!$&'()*+,;=%]+\",\"\",text) #url removed\n",
    "    processed = re.sub(r\"&[a-zA-Z0-9]+;\", \"\",processed) # html entity removal\n",
    "    processed = re.sub(r\"@\\w+\",\"\",processed) #@user mentions handling\n",
    "    processed = re.sub(r\"(\\.|!|\\?)\",\" \",processed)\n",
    "    processed = re.sub(r\"[^A-Za-z0-9 ]\",\"\",processed) # removing alphanumeric characters excluding space\n",
    "    processed = re.sub(r\"\\s+\",\" \",processed)   # white space removed\n",
    "    processed = re.sub(r\"\\b[0-9]+\\b\",\"\",processed)  # numbers removed\n",
    "\n",
    "    return re.sub(r\"\\b[a-z0-9]\\b\",\"\",processed)\n",
    "def remove_stopwords(texts):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [text for text in texts.split() if text not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def get_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN \n",
    "    \n",
    "def lemmatize(text):\n",
    "    texts = text.split()\n",
    "    lm = nltk.stem.WordNetLemmatizer()\n",
    "    pos_tags = nltk.pos_tag(texts)\n",
    "    words = [lm.lemmatize(word, pos=get_pos(tag)) for word, tag in pos_tags]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = regex_process(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cousin didnt know Friday movie lol'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sample\n",
    "example = '@DaniBooThang @indeliblemarq__ my cousin didnt know https://t.co/cDirIVdAEC from the Friday movies lol'\n",
    "preprocess(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, we need to load the training set, the development set and the test set. For better classification results, we need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "prediction_dict = {}\n",
    "training_file = 'twitter-training-data.txt'\n",
    "dev_file = 'twitter-dev-data.txt'\n",
    "\n",
    "# format\n",
    "# tweetids = {'training':[id1,id2]}\n",
    "# tweetgts = {'training':[gts1,gts2]}\n",
    "  \n",
    "for dataset in [training_file] + [dev_file] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "    location = join('semeval-tweets', dataset)\n",
    "    # write code to read in the datasets here\n",
    "    with open(location, encoding=\"utf8\") as d:\n",
    "        for line in d:\n",
    "            data_ = line.split('\\t') #each line data\n",
    "            tweetid = data_[0] #retriving tweet ids\n",
    "            gts = data_[1] #retriving sentiment of the tweet\n",
    "            tweet = preprocess(data_[2].lower()) #retriving tweet content and preprocessing it\n",
    "            #appending all the items for each of training, validation and test dataset\n",
    "            data[dataset].append(line)  \n",
    "            tweets[dataset].append(tweet)\n",
    "            tweetids[dataset].append(tweetid)\n",
    "            tweetgts[dataset].append(gts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'negative', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['felt privilege play foo fighter song guitar today one plectrum gig saturday',\n",
       " 'pakistan may islamic country der lot true muslim india love country sacrifice',\n",
       " 'happy birthday cool golfer bali may become cooler cooler everyday stay humble little sister xx',\n",
       " 'tmills go tucson 29th thursday',\n",
       " 'hmmmmm blacklivesmatter matter like rise kid disgrace']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweetgts[dataset][:20])\n",
    "tweets[training_file][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction using Bag of Words and TF-IDF \n",
    "\n",
    "Bag of Words: It represents each tweet as a vector of word frequencies. But is does not capture the context and meaning of words\n",
    "\n",
    "Term Frequency-Inverse Document Frequency: It represents each tweet as a weighted word frequency vector. These weights reflects the significance of words in the tweet and corpus of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def feature_extraction(type, train, test, datatype):\n",
    "    types = {'bow': CountVectorizer(), 'tf-idf': TfidfVectorizer()}\n",
    "    vectorizer = types.get(type)\n",
    "    X_train = vectorizer.fit_transform(train)\n",
    "    X_test = vectorizer.transform(test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a vocabulary to get all the unique words in the data and creating a word index with words as keys and unique integers as values for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# vocabulary creation\n",
    "def get_vocab(tweets):\n",
    "    tokenized_tweets = [tweet.split() for tweet in tweets]\n",
    "    word_counts = Counter([word for tweet in tokenized_tweets for word in tweet])\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    return vocab\n",
    "\n",
    "# creating word index dictionary\n",
    "def get_word_index(vocab):\n",
    "    word_idx = {'pad':0,'<unk>':1}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_idx[word] = i + 2 # we add 1 to reserve 0 for padding\n",
    "    return word_idx\n",
    "\n",
    "vocab = get_vocab(tweets[training_file])\n",
    "word_index = get_word_index(vocab)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading glove data and creating a dictionary with words as keys and vectors as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading GloVe embeddings\n",
    "glove={}\n",
    "location = join('glove.6B.100d.txt')\n",
    "with open(location, encoding=\"utf8\") as d:\n",
    "    for line in d:\n",
    "        data_ = line.split()\n",
    "        word = str(data_[0])\n",
    "        vector = np.asarray(data_[1:], dtype='float32') \n",
    "        glove[word] = vector\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an embedding matrix by mapping every word in word_index dictionary to corresponding array in glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 5000\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim)) \n",
    "for token, i in word_index.items():\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i<max_words:\n",
    "        if token in glove:\n",
    "            embedding_matrix[i] = glove[token]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(100) #this\n",
    "    else:\n",
    "        break\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class to contain the datasets, tokenise tweets and convert it into vector of unique values by mapping words to word_index and labels to (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as Data\n",
    "\n",
    "max_words = 5000\n",
    "batch_size = 32\n",
    "maxlen = 30 # maximum length of each sequence\n",
    "idx_labels = {2:'positive', 0:'negative', 1:'neutral'}\n",
    "class MyDataset(Data.Dataset):\n",
    "\n",
    "    def __init__(self, features, classes):\n",
    "        self.features = features\n",
    "        self.classes = classes\n",
    "        self.class_mapping = {'positive':0, 'negative': 1, 'neutral':2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.__len__()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = nltk.word_tokenize(self.features[idx])\n",
    "        tweet_indices = [word_index[word] if word in word_index and word_index[word]<5000 else word_index['<unk>'] for word in tokens][:maxlen]\n",
    "        tweet_indices += [0] * (maxlen - len(tweet_indices)) # pad with 0's if sequence is shorter than maxlen\n",
    "        labels = [self.class_mapping[i] for i in self.classes]\n",
    "        tweet_indices = torch.tensor(tweet_indices, dtype = torch.long)\n",
    "        st_classes = torch.tensor(labels[idx], dtype = torch.long)\n",
    "        return tweet_indices, st_classes\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    # -- Disable the gradient --\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            preds = model(X)\n",
    "            loss  = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds    = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Defining the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, max_words, embedding_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(max_words, embedding_dim) #5000*100\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix)) #using pretrained embedding vector\n",
    "        self.embedding.weight.requires_grad = False  # freeze the embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,) #100*128\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes) #128*3\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x) #(20, 40, 100)\n",
    "        lstm_out, _ = self.lstm(embeds.float()) #[20, 128] for lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out[:, -1, :])# [20, 1]\n",
    "        return (out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set hyperparameters\n",
    "MAX_WORDS = 5000\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_CLASSES = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS =10\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "model = LSTMClassifier(embedding_matrix, HIDDEN_SIZE, MAX_WORDS, EMBEDDING_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam( model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(X, y, batch_size):\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        tweets, classes = zip(*batch)\n",
    "        tweets = nn.utils.rnn.pad_sequence(tweets, batch_first=True)\n",
    "        classes = torch.tensor(classes, dtype=torch.long)\n",
    "        return tweets, classes\n",
    "\n",
    "    train_data = MyDataset(X,y)\n",
    "    return DataLoader(train_data,batch_size,collate_fn=collate_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def collate_fn(batch):\n",
    "    tweets, classes = zip(*batch)\n",
    "    tweets = nn.utils.rnn.pad_sequence(tweets, batch_first=True)\n",
    "    classes = torch.tensor(classes, dtype=torch.long)\n",
    "    return tweets, classes\n",
    "\n",
    "train_data = MyDataset(tweets[training_file],tweetgts[training_file])\n",
    "dev_data = MyDataset(tweets[dev_file],tweetgts[dev_file])\n",
    "train_loader = DataLoader(train_data,batch_size,collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_data,batch_size,collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Training Loop. Here, scheduler was used to gradually decrease the learning rate and find the best one out of them and after finding it out, it was commented out and the learning rate obtained(1e-3) is kept as a constant for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(model, loss_fn, optimizer, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = []\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs,labels)\n",
    "            loss = loss_fn(outputs, labels).to(device)\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # scheduler.step()\n",
    "        print(\"Epoch \",epoch,\" - Train Loss : {:.3f}\".format(torch.tensor(running_loss).mean()))\n",
    "        # print(\"Scheduler learning rate {:.3f}\".format(scheduler.get_last_lr()[0])) \n",
    "        # scheduler was used to gradually decrease the learning rate and find the best one out of them\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Prediction Code for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the Test Set \n",
    "import gc\n",
    "def predict(model, loader):\n",
    "    Y_preds = []\n",
    "    for X, Y in loader:\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "    gc.collect()\n",
    "    Y_preds = torch.cat(Y_preds)\n",
    "    return F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy() # logits to prob distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [06:13<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  - Train Loss : 0.371\n",
      "Valid Loss : 1.140\n",
      "Valid Acc  : 0.624\n"
     ]
    }
   ],
   "source": [
    "# training_loop(model, criterion, optimizer, train_loader, dev_loader, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "We will be creating three different classifiers for this project. For each classifier, we choose between the bag-of-word features and the word-embedding-based features. Each classifier we will be evaluating over validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training naive-bayes\n",
      "semeval-tweets\\twitter-dev-data.txt (bow-naive-bayes): 0.595\n",
      "            positive  negative  neutral\n",
      "positive    0.609     0.080     0.311     \n",
      "negative    0.058     0.606     0.335     \n",
      "neutral     0.227     0.141     0.632     \n",
      "\n",
      "Training naive-bayes\n",
      "semeval-tweets\\twitter-dev-data.txt (tf-idf-naive-bayes): 0.542\n",
      "            positive  negative  neutral\n",
      "positive    0.610     0.081     0.309     \n",
      "negative    0.063     0.656     0.281     \n",
      "neutral     0.249     0.162     0.589     \n",
      "\n",
      "Unknown classifier namenaive-bayes\n",
      "Training svm\n",
      "semeval-tweets\\twitter-dev-data.txt (bow-svm): 0.596\n",
      "            positive  negative  neutral\n",
      "positive    0.677     0.054     0.269     \n",
      "negative    0.073     0.629     0.297     \n",
      "neutral     0.228     0.153     0.619     \n",
      "\n",
      "Training svm\n",
      "semeval-tweets\\twitter-dev-data.txt (tf-idf-svm): 0.609\n",
      "            positive  negative  neutral\n",
      "positive    0.663     0.051     0.287     \n",
      "negative    0.051     0.629     0.321     \n",
      "neutral     0.239     0.144     0.617     \n",
      "\n",
      "Training LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:40<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  - Train Loss : 1.007\n",
      "Valid Loss : 0.939\n",
      "Valid Acc  : 0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:34<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  - Train Loss : 0.844\n",
      "Valid Loss : 0.801\n",
      "Valid Acc  : 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:37<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2  - Train Loss : 0.782\n",
      "Valid Loss : 0.770\n",
      "Valid Acc  : 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [06:06<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3  - Train Loss : 0.744\n",
      "Valid Loss : 0.760\n",
      "Valid Acc  : 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:38<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4  - Train Loss : 0.701\n",
      "Valid Loss : 0.767\n",
      "Valid Acc  : 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:55<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5  - Train Loss : 0.650\n",
      "Valid Loss : 0.800\n",
      "Valid Acc  : 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [07:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6  - Train Loss : 0.590\n",
      "Valid Loss : 0.842\n",
      "Valid Acc  : 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:51<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7  - Train Loss : 0.525\n",
      "Valid Loss : 0.915\n",
      "Valid Acc  : 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:32<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8  - Train Loss : 0.464\n",
      "Valid Loss : 0.982\n",
      "Valid Acc  : 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [05:32<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9  - Train Loss : 0.415\n",
      "Valid Loss : 1.051\n",
      "Valid Acc  : 0.618\n"
     ]
    }
   ],
   "source": [
    "# Buiding traditional sentiment classifiers  \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "params = {'svm':{'bow':0.1,'tf-idf':1},'naive-bayes':{'bow':1,'tf-idf':0.1}}\n",
    "\n",
    "for classifier in ['naive-bayes','svm', 'LSTM']:\n",
    "    for features in ['bow', 'tf-idf', 'embedding']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'svm':\n",
    "            if features == 'embedding':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "            svm_classifier = LinearSVC(C=params.get(classifier).get(features))\n",
    "        elif classifier == 'naive-bayes' and features != 'embedding':\n",
    "            if features == 'embedding':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "            nb = MultinomialNB(alpha=params.get(classifier).get(features))\n",
    "        elif classifier == 'LSTM' :\n",
    "            if features == 'bow' or features == 'tf-idf':\n",
    "                continue\n",
    "            # write the LSTM classifier here\n",
    "            print('Training ' + classifier)\n",
    "            # Instantiate the model\n",
    "            model = LSTMClassifier(embedding_matrix, HIDDEN_SIZE, MAX_WORDS, EMBEDDING_DIM, NUM_CLASSES).to(device)\n",
    "            # Loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam( model.parameters(), lr=LEARNING_RATE)\n",
    "            # scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "            train_loader = get_dataset(tweets[training_file],tweetgts[training_file],batch_size)\n",
    "            data_loader = get_dataset(tweets[testset],tweetgts[testset],batch_size)\n",
    "            training_loop(model, criterion, optimizer, train_loader, data_loader, NUM_EPOCHS)\n",
    "            \n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "        dev = ['twitter-dev-data.txt']\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "            if features != 'embedding':\n",
    "                X_train, X_test = feature_extraction(features,tweets[training_file],tweets[testset],'test')\n",
    "            if classifier == 'svm' and features != 'embedding':\n",
    "                svm_classifier.fit(X_train, tweetgts[training_file])\n",
    "                y_pred = svm_classifier.predict(X_test)\n",
    "            elif classifier == 'naive-bayes' and features != 'embedding':\n",
    "                nb.fit(X_train, tweetgts[training_file])\n",
    "                y_pred = nb.predict(X_test)\n",
    "            elif classifier == 'LSTM':\n",
    "                if features == 'bow' or 'tf-idf':\n",
    "                    continue\n",
    "                else:\n",
    "                    data_loader = get_dataset(tweets[testset],tweetgts[testset],batch_size)\n",
    "                    y = predict(model, data_loader)\n",
    "                    y_pred = [idx_labels[i] for i in y]\n",
    "            for k,v in zip(tweetids[testset],y_pred):\n",
    "                id_preds[k] = v\n",
    "            \n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)\n",
    "            confusion(id_preds, testset_path, features + '-' + classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets\\twitter-dev-data.txt (tf-idf-LSTM): 0.000\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_dataset(tweets[testset],tweetgts[testset],batch_size)\n",
    "y = predict(model, data_loader)\n",
    "y_pred = [idx_labels[i] for i in y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Code for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Tested hyper parameters - 'C': [0.1, 1, 10, 100, 1000], \n",
    "# # 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf', 'sigmoid','poly']}\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     # ('tfidf', TfidfVectorizer()),\n",
    "#     ('clf', SVC(kernel = 'poly'))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'clf__C': [0.1, 1, 10, 100, 1000], \n",
    "# 'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'clf__kernel': ['rbf', 'sigmoid','poly']\n",
    "#     # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "#     # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "#     # 'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "#     # 'nb__alpha': [0.001,0.0001,0.1, 1.0, 10.0],\n",
    "#     # 'clf__C': [0.1, 1, 10, 100, 1000], \n",
    "# }\n",
    "# scoring = 'f1_macro'\n",
    "# # X_train,Xtest = feature_extraction('bow',tweets[training_file],tweets[testsets[0]],'train')\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(tweetgts[training_file])\n",
    "# # Initialize the GridSearchCV object\n",
    "# grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "# print(\"Best score: \", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
